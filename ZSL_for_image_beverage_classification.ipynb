{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pjwQxa7oC7X"
      },
      "source": [
        "# Tutorial: Zero-Shot Learning for Beverage Classification in Images\n",
        "\n",
        "In this tutorial we are going to implement Zero-Shot Learning (ZSL) using two popular deep learning models: CLIP (Contrastive Language-Image Pre-training) and LLaVA (Large Language and Vision Assistant model). This notebook is available in two forms:\n",
        "\n",
        "1.   [Online (Google Colab)](https://colab.research.google.com/github/ltu-capr/zsl-image-tutorial/blob/master/ZSL_for_image_beverage_classification.ipynb): For experimenting on Google's free platform without installing anything on your computer.\n",
        "2.   [Offline (Jupyter Notebook)](https://github.com/ltu-capr/zsl-image-tutorial): For experimenting locally on your own computer. This takes some additional setup, but is the best option for working with sensitive data.\n",
        "\n",
        "To run the code in a cell, either click inside it and press Ctrl + Enter, or click the 'play' button to the left of the cell.\n",
        "\n",
        "*The ZSL models at the core of this notebook run much faster with graphics processing unit (GPU) acceleration. If you are in Google Colab, you can enable GPU acceleration in the settings by going to Runtime > Change runtime type > Hardware accelerator (select \"GPU\").*\n",
        "\n",
        "*To support most machines, this notebook uses a quantised version of LLaVA, that reduces the memory requirements at a small cost to accuracy. As such, a GPU is **required** to use LLaVA in this notebook*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aVaTkcMgtu0"
      },
      "source": [
        "## Example scenario: Beverage Classification in Images\n",
        "\n",
        "In this scenario, we want to classify the type of beverage depicted in an image from a set of predefined categories, and thus, determine if an alcoholic beverage is present.\n",
        "\n",
        "The candidate beverage types we aim to classify are:\n",
        "\n",
        "*    Beer\n",
        "*    Wine\n",
        "*    Water\n",
        "*    Coffee\n",
        "*    Tea\n",
        "*    No Beverage\n",
        "\n",
        "To approach this, we will use two popular deep learning models that operate in different ways:\n",
        "\n",
        "* CLIP (Contrastive Language-Image Pre-training)\n",
        "* LLaVA (Large Language and Vision Assistant)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooNIV_sjpjeY"
      },
      "source": [
        "### Cell 0.0: Install software package requirements\n",
        "\n",
        "\n",
        "*   bitsandbytes and accelerate are used to quantise the LLaVA model, such that we can run it on machines with smaller amounts of available memory.\n",
        "*   matplotlib is used for visualising the confusion matrix and examples in the dataset.\n",
        "*   numpy is used for efficient numerical computations.\n",
        "*   Pandas is used to load and save data in CSV (comma separated value) format.\n",
        "*   PyTorch and Transformers are used to run the models (torchvision is used by Transformers to preprocess the images).\n",
        "*   tqdm is used to show progress bars.\n",
        "*   scikit-learn is used for metric computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uafzJLsJsbqH"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate==1.2.0 bitsandbytes==0.45.0 matplotlib numpy pandas transformers==4.46.0 tqdm scikit-learn\n",
        "!pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu124"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ugA61loYdj"
      },
      "source": [
        "### Cell 0.1: Import essential modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFnIfU23mRhY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import CLIPProcessor, CLIPModel, LlavaNextProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywX7vzE4cBvf"
      },
      "outputs": [],
      "source": [
        "if not torch.cuda.is_available():\n",
        "    print('[WARNING] No GPU was detected. CLIP will run without a GPU, however a GPU is required to run the LLaVA model.\\n' \\\n",
        "          'If using Google Colab, you can enable the GPU by selecting Runtime > Change runtime type > Hardware accelerator (select \"GPU\")')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIKy7nbI4UK4"
      },
      "source": [
        "### Cell 0.2 Define useful functions\n",
        "Here we define some useful functions to help compute metrics we want to report for both models. These functions will be used in Cells 1.4 and 2.4, and in the Prompt Engineering (optional) section. The code here may look daunting, and it is okay if you don't fully understand it right away. If you prefer, you can simply run the code cell and move on to the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ULw0KTS4Tsm"
      },
      "outputs": [],
      "source": [
        "def per_class_sensitivity_specificity(ground_truth_labels, predicted_labels, label_names):\n",
        "    \"\"\"Computes per-class sensitivity (recall) and specificity metrics.\n",
        "\n",
        "    Metrics returned in order of label_names.\n",
        "    \"\"\"\n",
        "    sensitivity_scores, specificity_scores = [], []\n",
        "    for label_name in label_names:\n",
        "        # Turn into binary problem. recall_score returns specificity then sensitivity\n",
        "        specificity, sensitivity = recall_score(\n",
        "            np.array(ground_truth_labels) == label_name, np.array(predicted_labels) == label_name,\n",
        "            pos_label=True, average=None, zero_division=np.nan)\n",
        "        sensitivity_scores.append(sensitivity)\n",
        "        specificity_scores.append(specificity)\n",
        "    return sensitivity_scores, specificity_scores\n",
        "\n",
        "\n",
        "def generate_metric_report(ground_truth_labels, predicted_labels, label_names):\n",
        "    \"\"\"Computes and displays a report containing a collection of metrics.\n",
        "\n",
        "    Per-class metrics include:\n",
        "    - Precision\n",
        "    - Recall (sensitivity)\n",
        "    - Specificity\n",
        "    - F1-Score (harmonic mean of precision and recall)\n",
        "\n",
        "    Overall metrics include:\n",
        "    - Accuracy\n",
        "    - Unweighted Average Recall (UAR)\n",
        "    \"\"\"\n",
        "    # Determine the length of the longest label name to format the table.\n",
        "    len_longest_label = max(len(label_name) for label_name in label_names)\n",
        "\n",
        "    # Compute per-class metrics.\n",
        "    per_class_precision = precision_score(ground_truth_labels, predicted_labels, labels=label_names, average=None, zero_division=np.nan)\n",
        "    per_class_recall, per_class_specificity = per_class_sensitivity_specificity(ground_truth_labels, predicted_labels, label_names=label_names)\n",
        "    per_class_f1_score = f1_score(ground_truth_labels, predicted_labels, labels=label_names, average=None, zero_division=np.nan)\n",
        "\n",
        "    # Display the per-class summary.\n",
        "    print(f'{\"\":^{len_longest_label}s}    {\"precision\":>9s}    {\"recall\":>6s}    {\"specificity\":>11s}    {\"f1-score\":>9s}    {\"support\":>6s}\\n')\n",
        "    for label_idx, label_name in enumerate(label_names):\n",
        "        print(f'{label_name:>{len_longest_label}s}    {per_class_precision[label_idx]:>9.2f}    {per_class_recall[label_idx]:>6.2f}    {per_class_specificity[label_idx]:>11.2f}    {per_class_f1_score[label_idx]:>9.2f}    {ground_truth_labels.count(label_name):>6d}')\n",
        "    print('\\n')\n",
        "\n",
        "    # Compute overall metrics.\n",
        "    accuracy = accuracy_score(ground_truth_labels, predicted_labels)\n",
        "    uar = recall_score(ground_truth_labels, predicted_labels, labels=label_names, average='macro', zero_division=np.nan)\n",
        "\n",
        "    # Display the overall metrics.\n",
        "    print(f'{\"Number of Examples\":>35s}: {len(ground_truth_labels)}')\n",
        "    print(f'{\"Overall Accuracy\":>35s}: {accuracy:.2%}')\n",
        "    print(f'{\"Unweighted Average Recall (UAR)\":>35s}: {uar:.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiHKZs_KoYgE"
      },
      "source": [
        "### Cell 0.3: Load the \"beverage\" dataset\n",
        "\n",
        "To evaluate model performance across different beverage types and scenes, we have constructed a dataset consisting of the following beverages:\n",
        "\n",
        "**Alcoholic Beverages**\n",
        "*   Beer bottle\n",
        "*   Beer cup\n",
        "*   Red wine glass\n",
        "*   White wine glass\n",
        "\n",
        "\n",
        "**Non-Alcoholic Beverages**\n",
        "*   Coffee cup\n",
        "*   Coffee plunger\n",
        "*   Tea cup\n",
        "*   Water bottle\n",
        "*   Water cup\n",
        "\n",
        "\n",
        "Multiple images of each beverage have been captured across five distinct scenes, both indoor and outdoor, with the beverage positioned at three different distances from the camera: foreground, midground, and background. The image below shows examples of the images contained in the dataset:\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/ltu-capr/zsl-image-tutorial/blob/main/dataset_example.jpg?raw=1\" width=\"70%\"/>\n",
        "</div>\n",
        "\n",
        "In total, for each of the nine beverage types, images have been taken across five different scenes and at three focal distances, resulting in 15 images per beverage. This leads to a complete dataset of 135 images (9 beverages $\\times$ 15 images each).\n",
        "\n",
        "**Note for offline notebook version**: You may encounter an error when downloading the image dataset when running the below code cell. If so:\n",
        "1. Manually download the dataset from [here](https://github.com/ltu-capr/zsl-image-tutorial/raw/refs/heads/main/Data/Images.zip).\n",
        "2. Extract the downloaded ZIP folder and place the folder in the same location as this notebook. The name of the folder should be 'Images', and inside the folder should be 135 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcp0m4-O0_jq"
      },
      "outputs": [],
      "source": [
        "# Download and extract the zipped image dataset.\n",
        "!wget -O images.zip 'https://github.com/ltu-capr/zsl-image-tutorial/raw/refs/heads/main/Data/Images.zip'\n",
        "!unzip -o -q images.zip -d ./Images\n",
        "\n",
        "# Load the CSV file containing the labels for each image.\n",
        "# Here we are giving the URL for a sample file that we've made publicly\n",
        "# available on the Internet.\n",
        "labels_location = 'https://raw.githubusercontent.com/ltu-capr/zsl-image-tutorial/main/Data/images_labelled.csv'\n",
        "beverage_dataframe = pd.read_csv(labels_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vQlcHjJJnaj"
      },
      "source": [
        "### Cell 0.4: Visualise the \"beverage\" dataset\n",
        "Here we visualise a few of the examples from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42agiLVw2N8w"
      },
      "outputs": [],
      "source": [
        "# Define how many images to show.\n",
        "rows, cols = 3, 6\n",
        "num_images = rows * cols\n",
        "\n",
        "# Create a 3x6 grid for displaying images.\n",
        "fix, axes = plt.subplots(3, 6, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for index, sample in beverage_dataframe[:num_images].iterrows():\n",
        "    # Load the image and create a small thumbnail.\n",
        "    im = Image.open('Images/' + sample['image_name'] + '.jpg')\n",
        "    im.thumbnail((256, 256), Image.Resampling.LANCZOS)\n",
        "\n",
        "    # Display the image, setting the title as the beverage type and focal location.\n",
        "    axes[index].imshow(im)\n",
        "    axes[index].set_title(f'{sample[\"beverage\"]}/{sample[\"position\"]}')\n",
        "\n",
        "    # Hide the axis markers.\n",
        "    axes[index].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "\n",
        "# Display the plot.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6S86q7qoYit"
      },
      "source": [
        "## Example 1: ZSL Classification of Beverages in Images Using CLIP\n",
        "\n",
        "CLIP works by taking an image and a collection of descriptors, and generates an image-text similarity score for each image-descriptor pair. With this, we can provide CLIP an image along with a set of candidate labels, and generate a prediction by using the candidate label with the highest image-text similarity score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk8x7uQFoYlK"
      },
      "source": [
        "### Cell 1.0: Initialise the CLIP model\n",
        "\n",
        "Initialise the CLIP model for use in zero-shot image classification. It may take a while for the model to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VNvD9LjArfUx"
      },
      "outputs": [],
      "source": [
        "# Use the GPU if it's available.\n",
        "clip_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load the pre-trained model.\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(clip_device)\n",
        "\n",
        "# Load an object used to prepare images in the right way required by CLIP\n",
        "# This will: resize, centrecrop, and normalise the images.\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BSyeK9doYns"
      },
      "source": [
        "### Cell 1.1: Initialise classification labels\n",
        "\n",
        "In order to perform classification, we must nominate candidate labels for the model to choose between. In this scenario we have 6x labels, however you can choose as many labels as you need.\n",
        "\n",
        "It is possible to further engineer the text in these labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX6QoZ2ruCSu"
      },
      "outputs": [],
      "source": [
        "candidate_labels = [\n",
        "    'A photo containing beer',\n",
        "    'A photo containing wine',\n",
        "    'A photo containing coffee',\n",
        "    'A photo containing tea',\n",
        "    'A photo containing water',\n",
        "    'A photo containing no drinks'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeOCo5CSuDth"
      },
      "source": [
        "### Cell 1.2: Make model predictions\n",
        "\n",
        "Here we generate predictions for all images in our dataset.\n",
        "\n",
        "In this example we also implement *batching*, enabling the model to process multiple images simultaneously. This is especially useful for larger datasets, as it can reduce computational overhead and speed up predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2FWBMbavl09"
      },
      "outputs": [],
      "source": [
        "# Set the batch size (how many images to process at once).\n",
        "clip_batch_size = 10\n",
        "\n",
        "# Generate prediction results for all images in our dataset.\n",
        "all_results = []\n",
        "for index in tqdm(range(0, len(beverage_dataframe), clip_batch_size)):\n",
        "    # Extract the next set of data to process.\n",
        "    next_data = beverage_dataframe.iloc[index:index + clip_batch_size]\n",
        "\n",
        "    # Open the images.\n",
        "    images = [Image.open('Images/' + sample['image_name'] + '.jpg') for _, sample in next_data.iterrows()]\n",
        "\n",
        "    # Preprocess the data to the correct format.\n",
        "    inputs = clip_processor(\n",
        "        text=candidate_labels,\n",
        "        images=images, return_tensors='pt', padding=True\n",
        "    ).to(clip_device)\n",
        "\n",
        "    # Get the model outputs.\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "\n",
        "    # Get the logits per-image (the image-text similarity score to each candidate label).\n",
        "    image_logits = outputs.logits_per_image.cpu()\n",
        "\n",
        "    # Convert logits per-image into probabilities with a softmax, then convert to a list.\n",
        "    image_probs = image_logits.softmax(dim=1)\n",
        "    image_probs = list(torch.unbind(image_probs, dim=0))\n",
        "\n",
        "    # Store the probabilities.\n",
        "    all_results.extend(image_probs)\n",
        "\n",
        "    # Visualise last result in batch as model is running.\n",
        "    last_image = images[-1]\n",
        "    last_image_name = next_data.iloc[-1]['image_name']\n",
        "    last_image_probs = all_results[-1]\n",
        "\n",
        "    # Determine the index of the highest probability.\n",
        "    highest_idx = torch.argmax(last_image_probs)\n",
        "\n",
        "    # Prepare the image for displaying\n",
        "    thumb_im = last_image.copy()\n",
        "    thumb_im.thumbnail((256, 256), Image.Resampling.LANCZOS)\n",
        "    display(thumb_im)\n",
        "\n",
        "    print(f'Most likely label for {last_image_name}: {candidate_labels[highest_idx]} (probability: {100 * last_image_probs[highest_idx].item():.1f}%)')\n",
        "    print(f'Per-class probabilities:')\n",
        "    for lbl_idx, label in enumerate(candidate_labels):\n",
        "        print(f'\\t{label}: {100 * last_image_probs[lbl_idx]:.1f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgqkJrkwQS-K"
      },
      "source": [
        "### Cell 1.3 Save the model predictions\n",
        "\n",
        "This code prepares an output CSV file containing model predictions which can be used for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0NRDvJMQnzX"
      },
      "outputs": [],
      "source": [
        "def save_clip_predictions(file_name, dataframe, all_results, candidate_labels, actual_label_column=None, label_class_map=None):\n",
        "    # Arrange the results in tabular form with neat columns.\n",
        "    rows = []\n",
        "    for result, (index, sample) in zip(all_results, dataframe.iterrows()):\n",
        "        scores_as_percentages = [round(score * 100, 2) for score in result.tolist()]\n",
        "        row = {'image_name': sample['image_name'], **dict(zip(candidate_labels, scores_as_percentages))}\n",
        "        rows.append(row)\n",
        "    results_df = pd.DataFrame(rows, columns=['image_name', *candidate_labels])\n",
        "    results_df['predicted_label'] = results_df[candidate_labels].idxmax(axis=1)\n",
        "\n",
        "    if actual_label_column is not None and actual_label_column in dataframe.columns:\n",
        "        if label_class_map is not None:\n",
        "            results_df['predicted_class'] = results_df['predicted_label'].map(label_class_map)\n",
        "        else:\n",
        "            results_df['predicted_class'] = results_df['predicted_label'].str.replace('A photo containing ', '')\n",
        "        results_df['actual_class'] = dataframe[actual_label_column]\n",
        "\n",
        "    # Save output to a CSV file.\n",
        "    os.makedirs('Outputs', exist_ok=True)\n",
        "    output_file_name = os.path.join('Outputs', file_name)\n",
        "    results_df.to_csv(output_file_name, index=False)\n",
        "\n",
        "    try:\n",
        "        # If we are on Google Colab, download the results.\n",
        "        from google.colab import files\n",
        "        files.download(output_file_name)\n",
        "    except ModuleNotFoundError:\n",
        "        # If we are not on Google Colab, show the output file location.\n",
        "        print('Output file saved:')\n",
        "        print(os.path.abspath(output_file_name))\n",
        "\n",
        "\n",
        "save_clip_predictions('beverage_classification_clip_predictions.csv',\n",
        "                      beverage_dataframe, all_results, candidate_labels,\n",
        "                      'beverage_type')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EcxQEsOs9Qy"
      },
      "source": [
        "### Cell 1.4. Measure the accuracy of model predictions (optional)\n",
        "\n",
        "Zero-shot learning does not require hand-annotated labels to generate predictions, but they can be used to validate the model's accuracy. Here we compare the model's outputs with hand-annotated (ground truth) labels. If you don't have hand-annotated labels for your data, skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDATsLJdLKyK"
      },
      "source": [
        "#### Cell 1.4a. Metric computation\n",
        "In the code cell below, we will generate a table that summarises the precision, recall, specificity and F1 score for each beverage type individually, in addition to other statistics summarising the overall model performance, such as the overall accuracy and unweighted average recall (UAR).\n",
        "\n",
        "In the generated report, *support* refers to the number of ground truth labels belonging to that class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBRRYtkdAaiB"
      },
      "outputs": [],
      "source": [
        "# Put the predictions and ground truth into the right format.\n",
        "predicted_labels = [candidate_labels[torch.argmax(result)] for result in all_results]\n",
        "ground_truth_labels = list('A photo containing ' + beverage_dataframe['beverage_type'])\n",
        "\n",
        "# Produce the metrics report.\n",
        "generate_metric_report(ground_truth_labels, predicted_labels, label_names=candidate_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL7WezTqVsze"
      },
      "source": [
        "These statistics show that CLIP only achieves an overall accuracy of 34.07%. Though there is no baseline for accuracy, this is still a poor result, and suggests further engineering of the classification labels may be needed, or a different model may be more appropriate for this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fws7vAZpLNdo"
      },
      "source": [
        "#### Cell 1.4b. Confusion matrix\n",
        "\n",
        "We can analyse the model's performance in a more detailed manner by visualising a confusion matrix. A confusion matrix shows how many dataset examples there are for each possible pair of true and predicted labels. Numbers which do not lie on the main diagonal of the matrix correspond to misclassifications. By inspecting the classification matrix, we can quickly observe specific classes that the model is performing poorly on. For example, in this case we can see the model is best at correctly identifying photos containing beer, wine, and coffee, however struggles with other beverage types, such as water."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZVbQpWLVs8W"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=candidate_labels, xticks_rotation='vertical', cmap='Blues', colorbar=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nePUCwnvWsip"
      },
      "source": [
        "#### Cell 1.4c. Foreground/midground/background performance\n",
        "\n",
        "We can take a closer look at what other factors might be causing this misclassification. For instance, we can look at the classification performance in foreground, midground, and background. In this case, we can see that the model is much better at making correct predictions when the beverage is in the foreground as opposed to the midground or background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ebePk2S4O7x"
      },
      "outputs": [],
      "source": [
        "for position in ('foreground', 'midground', 'background'):\n",
        "    predicted_labels = []\n",
        "    ground_truth_labels = []\n",
        "    for result, (index, sample) in zip(all_results, beverage_dataframe.iterrows()):\n",
        "        if sample['position'] == position:\n",
        "            predicted_labels.append(candidate_labels[torch.argmax(result)])\n",
        "            ground_truth_labels.append('A photo containing ' + sample['beverage_type'])\n",
        "\n",
        "    accuracy = accuracy_score(ground_truth_labels, predicted_labels)\n",
        "    print(f'Accuracy for {position}: {accuracy:.2%}', flush=True)\n",
        "\n",
        "    ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=candidate_labels, xticks_rotation='vertical', cmap='Blues', colorbar=False);\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_nqKJfW8u6F"
      },
      "source": [
        "#### Cell 1.4d. Alcohol vs. not-alcohol evaluation\n",
        "\n",
        "Finally, we can take a look at how well the model performed at correctly classifying whether an image contains an alcoholic beverage (beer or wine). If no beverage was detected, we classify this as no alcohol present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxLrD-Hl8ui0"
      },
      "outputs": [],
      "source": [
        "# Put the predictions and ground truth into the right format.\n",
        "predicted_labels = [candidate_labels[torch.argmax(result)] for result in all_results]\n",
        "predicted_labels = ['alcohol' if 'beer' in label or 'wine' in label else 'not alcohol' for label in predicted_labels]\n",
        "ground_truth_labels = list(beverage_dataframe['alcohol_notalcohol'])\n",
        "\n",
        "# Produce the metrics report.\n",
        "generate_metric_report(ground_truth_labels, predicted_labels, label_names=['alcohol', 'not alcohol'])\n",
        "\n",
        "# Generate the confusion matrix.\n",
        "ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=['alcohol', 'not alcohol'], xticks_rotation='vertical', cmap='Blues', colorbar=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoogcj1ss9TS"
      },
      "source": [
        "## Example 2 - ZSL Classification of Beverages in Images Using LLaVA\n",
        "\n",
        "LLaVA is a large multimodal model that can accept both images and text as input. With LLaVA, we can provide an image along with a question, such as \"What beverage is contained in the image?\" or \"Does the image contain an alcoholic beverage?\". To make the answer suitable for classification, we can also ask the model to restrict its response to a set of candidate categories or a simple yes or no."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8V3lJfOuNj9"
      },
      "source": [
        "### Cell 2.0 Initialise the LLaVA model\n",
        "\n",
        "Initialise the LLaVA model for use in zero-shot image classification. It will take a while for the model to download.\n",
        "\n",
        "LLaVA is quite a large model, which by default might not be able to run on most machines due to memory constraints. To support most machines, we will use a quantised version of the model, that reduces the memory requirements at a small cost to accuracy.\n",
        "\n",
        "This process requires the use of a GPU, meaning that a GPU is required to run the quantised version of LLaVA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FoRjpEhvh1LK"
      },
      "outputs": [],
      "source": [
        "# Must use the GPU when running a quantised LLaVA.\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError('Must have a GPU available to run a quantised version of LLaVA.')\n",
        "llava_device = 'cuda:0'\n",
        "\n",
        "# Specify which version of LLaVA we will use.\n",
        "model_id = 'llava-hf/llava-v1.6-mistral-7b-hf'\n",
        "\n",
        "# Set up parameters relating to model quantisation.\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Load the pre-trained model.\n",
        "llava_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
        "    model_id, quantization_config=quantization_config)\n",
        "\n",
        "# Load an object used to prepare data in the right way required by LLaVA.\n",
        "llava_processor = LlavaNextProcessor.from_pretrained(\n",
        "    model_id, patch_size=llava_model.config.vision_config.patch_size,\n",
        "    vision_feature_select_strategy=llava_model.config.vision_feature_select_strategy,\n",
        ")\n",
        "llava_model.generation_config.pad_token_id = llava_processor.tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63LAqulSB1qA"
      },
      "source": [
        "### Cell 2.1 Initialise prompt\n",
        "\n",
        "In order to perform classification we must first create a prompt for the LLaVA model. Here, we will ask the model to identify the presence of any specific beverages (beer, wine, coffee, tea, water) or none in the image. This approach allows us to handle the task as a multi-class classification problem, similar to what we did for CLIP.\n",
        "\n",
        "We also limit the maximum number of tokens (words or word parts) that LLaVA can generate in its response. Here, we set it to a low value (10) as we only expect a single-word answer, though this choice is less critical as our prompt specifically instructs LLaVA to respond with only one word. Raising this limit would let LLaVA generate longer responses, which can be useful if a more detailed answer is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN1YT0SPB-PU"
      },
      "outputs": [],
      "source": [
        "# Here we specify the prompt to give to LLaVA.\n",
        "prompt = 'What type of beverage does this image contain? Pick the most relevant one from this list: beer, wine, coffee, tea, water, none. Answer in one word.'\n",
        "\n",
        "# Here we specify how many tokens (words or word parts) can be returned by LLaVA. We restrict this to a small number.\n",
        "max_new_tokens = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYyn05Kyk6Nv"
      },
      "source": [
        "### Cell 2.2 Make model predictions\n",
        "\n",
        "Here we generate predictions for all images in our dataset.\n",
        "\n",
        "In this example we also implement *batching*, enabling the model to process multiple images simultaneously. This is especially useful for larger datasets, as it can reduce computational overhead and speed up predictions.\n",
        "\n",
        "Depending on the GPU used, a smaller batch size may be required due to memory constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEGkhjDklkrF"
      },
      "outputs": [],
      "source": [
        "# Set the batch size (how many images to process at once).\n",
        "llava_batch_size = 10\n",
        "\n",
        "# Generate prediction results for all images in our dataset.\n",
        "all_results = []\n",
        "for index in tqdm(range(0, len(beverage_dataframe), llava_batch_size)):\n",
        "    # Extract the next set of data to process.\n",
        "    next_data = beverage_dataframe.iloc[index:index + llava_batch_size]\n",
        "\n",
        "    # Open the images.\n",
        "    images = [Image.open('Images/' + sample['image_name'] + '.jpg') for _, sample in next_data.iterrows()]\n",
        "\n",
        "    # Create the formatted prompt for LLaVA.\n",
        "    llava_prompt = f'[INST] <image>\\n{prompt} [/INST]'\n",
        "\n",
        "    # Prepare the inputs to pass to LLaVA.\n",
        "    inputs = llava_processor(\n",
        "        text=[llava_prompt] * len(images),\n",
        "        images=images, return_tensors='pt'\n",
        "    ).to(llava_device)\n",
        "\n",
        "    # Get the model predictions.\n",
        "    with torch.no_grad():\n",
        "        outputs = llava_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Process all predictions.\n",
        "    for output in outputs:\n",
        "        # Get the generated text and extract just the response.\n",
        "        generated_text = llava_processor.decode(output.cpu(), skip_special_tokens=True)\n",
        "        response = generated_text.split(f'{prompt} [/INST]')[1].strip().lower()\n",
        "\n",
        "        # Store the response.\n",
        "        all_results.append(response)\n",
        "\n",
        "    # Visualise last result in batch as model is running.\n",
        "    last_image = images[-1]\n",
        "    last_image_name = next_data.iloc[-1]['image_name']\n",
        "    last_image_response = all_results[-1]\n",
        "\n",
        "    # Prepare the image for displaying\n",
        "    thumb_im = last_image.copy()\n",
        "    thumb_im.thumbnail((256, 256), Image.Resampling.LANCZOS)\n",
        "    display(thumb_im)\n",
        "\n",
        "    print(f'Image: {last_image_name}')\n",
        "    print(f'Prompt: {prompt}')\n",
        "    print(f'Response: {last_image_response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP8caeajppK2"
      },
      "source": [
        "### Cell 2.3 Save the model predictions\n",
        "\n",
        "This code prepares an output CSV file containing model predictions which can be used for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz8GMt95pzQ8"
      },
      "outputs": [],
      "source": [
        "def save_llava_predictions(file_name, dataframe, all_results, actual_label_column=None, response_class_map=None, default_class=None):\n",
        "    # Arrange the results in tabular form with neat columns.\n",
        "    results_df = pd.DataFrame([])\n",
        "    results_df['image_name'] = dataframe['image_name']\n",
        "    results_df['predicted_label'] = all_results\n",
        "\n",
        "    if actual_label_column is not None and actual_label_column in dataframe.columns:\n",
        "        if response_class_map is not None:\n",
        "            results_df['predicted_class'] = results_df['predicted_label'].map(lambda x: response_class_map.get(x, default_class))\n",
        "        else:\n",
        "            results_df['predicted_class'] = results_df['predicted_label']\n",
        "        results_df['actual_class'] = dataframe[actual_label_column]\n",
        "\n",
        "    # Save output to a CSV file.\n",
        "    os.makedirs('Outputs', exist_ok=True)\n",
        "    output_file_name = os.path.join('Outputs', file_name)\n",
        "    results_df.to_csv(output_file_name, index=False)\n",
        "\n",
        "    try:\n",
        "        # If we are on Google Colab, download the results.\n",
        "        from google.colab import files\n",
        "        files.download(output_file_name)\n",
        "    except ModuleNotFoundError:\n",
        "        # If we are not on Google Colab, show the output file location.\n",
        "        print('Output file saved:')\n",
        "        print(os.path.abspath(output_file_name))\n",
        "\n",
        "save_llava_predictions('beverage_classification_llava_predictions.csv',\n",
        "                       beverage_dataframe, all_results, 'beverage_type')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hJ_-1I06qWl"
      },
      "source": [
        "### Cell 2.4 Measure the accuracy of model predictions (optional)\n",
        "\n",
        "Zero-shot learning does not require hand-annotated labels to generate predictions, but they can be used to validate the model's accuracy. Here we compare the model's outputs with hand-annotated (ground truth) labels. If you don't have hand-annotated labels for your data, skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXbehHc0LuzZ"
      },
      "source": [
        "#### Cell 2.4a. Metric computation\n",
        "\n",
        "In the code cell below, we will generate a table that summarises the precision, recall, specificity and F1 score for each beverage type individually, in addition to other statistics summarising the overall model performance, such as the overall accuracy and unweighted average recall (UAR).\n",
        "\n",
        "In the generated report, *support* refers to the number of ground truth labels belonging to that class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnesNHDH6qj3"
      },
      "outputs": [],
      "source": [
        "# Put the predictions and ground truth into the right format.\n",
        "predicted_labels = all_results\n",
        "ground_truth_labels = list(beverage_dataframe['beverage_type'])\n",
        "\n",
        "# Produce the metrics report.\n",
        "generate_metric_report(ground_truth_labels, predicted_labels, label_names=('beer', 'wine', 'coffee', 'tea', 'water', 'none'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N3b3MIJ7Crg"
      },
      "source": [
        "These results look much better than for CLIP, with an overall accuracy of 79.26%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R821iuiuLyOB"
      },
      "source": [
        "#### Cell 2.4b. Confusion matrix\n",
        "\n",
        "We can analyse accuracy in a more detailed manner by visualising a confusion matrix. Here we can see the model does a really good job! In this case we can see the model often incorrectly classifies tea as coffee, however distinguishing between the two is very challenging!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXhnDq687B9B"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=('beer', 'wine', 'coffee', 'tea', 'water', 'none'), xticks_rotation='vertical', cmap='Blues', colorbar=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHj_I2yupWzQ"
      },
      "source": [
        "#### Cell 2.4c. Foreground/midground/background performance\n",
        "\n",
        "We can further analyse the classification performance in foreground, midground, and background. Similarly to CLIP, the model performs better when the beverage is contained in the foreground, and worse when in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxb-gmEApVxt"
      },
      "outputs": [],
      "source": [
        "for position in ('foreground', 'midground', 'background'):\n",
        "    predicted_labels = []\n",
        "    ground_truth_labels = []\n",
        "    for result, (index, sample) in zip(all_results, beverage_dataframe.iterrows()):\n",
        "        if sample['position'] == position:\n",
        "            predicted_labels.append(result)\n",
        "            ground_truth_labels.append(sample['beverage_type'])\n",
        "\n",
        "    accuracy = accuracy_score(ground_truth_labels, predicted_labels)\n",
        "    print(f'Accuracy for {position}: {accuracy:.2%}', flush=True)\n",
        "\n",
        "    ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=('beer', 'wine', 'coffee', 'tea', 'water', 'none'), xticks_rotation='vertical', cmap='Blues', colorbar=False);\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXfcOpohqZHs"
      },
      "source": [
        "#### Cell 2.4d. Alcohol vs. not-alcohol evaluation\n",
        "\n",
        "Finally, we can take a look at how well the model performed at correctly classifying whether an image contains an alcoholic beverage (beer or wine). If no beverage was detected, we classify this as no alcohol present. There were no instances where the model incorrectly labelled not alcohol as alcohol!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA0WfLmVqWEK"
      },
      "outputs": [],
      "source": [
        "# Put the predictions and ground truth into the right format.\n",
        "predicted_labels = ['alcohol' if label in {'beer', 'wine'} else 'not alcohol' for label in all_results]\n",
        "ground_truth_labels = list(beverage_dataframe['alcohol_notalcohol'])\n",
        "\n",
        "# Produce the metrics report.\n",
        "generate_metric_report(ground_truth_labels, predicted_labels, label_names=['alcohol', 'not alcohol'])\n",
        "\n",
        "# Generate the confusion matrix.\n",
        "ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=['alcohol', 'not alcohol'], xticks_rotation='vertical', cmap='Blues', colorbar=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPst5YkwwH13"
      },
      "source": [
        "## Prompt Engineering (optional)\n",
        "\n",
        "In this section, we present a more comprehensive analysis of our CLIP and LLaVA results by testing and evaluating different prompts. While this section is optional, it provides deeper insight into the prompt engineering process and demonstrates how different prompts can be compared."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzR87v5ZwcPX"
      },
      "source": [
        "### CLIP\n",
        "\n",
        "This section evaluates several sets of candidate labels for the CLIP model to compare how various sets of prompts impact model performance.\n",
        "\n",
        "Please make sure to run Cell 1.0 and Cell 1.3 before running this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xObPE72kMzlU"
      },
      "source": [
        "#### Candidate Labels\n",
        "\n",
        "The code cell below defines a list of different candidate label sets that will be passed to CLIP to make predictions on our validation dataset.\n",
        "\n",
        "Each item in the list is a dictionary with:\n",
        "* `name`: An identifier for the candidate label set. This name appears when summarising model performance and is used as the filename when storing predictions a CSV.\n",
        "* `labels`: A list of `(label, class)` pairs:\n",
        "  * `label` is the prompt given to CLIP.\n",
        "  * `class` is the ground truth class associated with that label.\n",
        "\n",
        "An example item is shown below:\n",
        "```\n",
        "{\n",
        "    'name': 'A photo containing',\n",
        "    'labels': [\n",
        "        ('A photo containing beer', 'beer'),\n",
        "        ('A photo containing wine', 'wine'),\n",
        "        ('A photo containing coffee', 'coffee'),\n",
        "        ('A photo containing tea', 'tea'),\n",
        "        ('A photo containing water', 'water'),\n",
        "        ('A photo containing no drinks', 'no drinks'),\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "If you would like to explore further prompt engineering on our validation dataset, feel free to add your own candidate label sets below or modify the existing ones as you see fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYJDQ_0PwR7a"
      },
      "outputs": [],
      "source": [
        "# Sets of candidate labels to be evaluated\n",
        "clip_label_set = [\n",
        "    # Candidate label set 1: Prefixing beverages by 'A photo containing'\n",
        "    {\n",
        "        'name': 'A photo containing',\n",
        "        'labels': [\n",
        "            ('A photo containing beer', 'beer'),\n",
        "            ('A photo containing wine', 'wine'),\n",
        "            ('A photo containing coffee', 'coffee'),\n",
        "            ('A photo containing tea', 'tea'),\n",
        "            ('A photo containing water', 'water'),\n",
        "            ('A photo containing no drinks', 'no drinks'),\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    # Candidate label set 2: Prefixing beverages by 'An image of'\n",
        "    {\n",
        "        'name': 'An image of',\n",
        "        'labels': [\n",
        "            ('An image of beer', 'beer'),\n",
        "            ('An image of wine', 'wine'),\n",
        "            ('An image of coffee', 'coffee'),\n",
        "            ('An image of tea', 'tea'),\n",
        "            ('An image of water', 'water'),\n",
        "            ('An image of no drinks', 'no drinks'),\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    # Candidate label set 3: Just listing the beverage type\n",
        "    {\n",
        "        'name': 'No Prefix',\n",
        "        'labels': [\n",
        "            ('beer', 'beer'),\n",
        "            ('wine', 'wine'),\n",
        "            ('coffee', 'coffee'),\n",
        "            ('tea', 'tea'),\n",
        "            ('water', 'water'),\n",
        "            ('no drinks', 'no drinks'),\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    # Candidate label set 4: Adding descriptions of the beverage receptacle\n",
        "    {\n",
        "        'name': 'Receptacle Description',\n",
        "        'labels': [\n",
        "            ('beer bottle or glass', 'beer'),\n",
        "            ('wine glass', 'wine'),\n",
        "            ('coffee mug or plunger', 'coffee'),\n",
        "            ('teacup', 'tea'),\n",
        "            ('water bottle or glass', 'water'),\n",
        "            ('no drinks', 'no drinks'),\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    # Candidate label set 5: Separate labels for each beverage receptacle\n",
        "    {\n",
        "        'name': 'Separate Receptacle Description',\n",
        "        'labels': [\n",
        "            ('beer bottle', 'beer'),\n",
        "            ('beer glass', 'beer'),\n",
        "            ('wine glass', 'wine'),\n",
        "            ('coffee mug', 'coffee'),\n",
        "            ('coffee plunger', 'coffee'),\n",
        "            ('teacup', 'tea'),\n",
        "            ('water bottle', 'water'),\n",
        "            ('water glass', 'water'),\n",
        "            ('no drinks', 'no drinks'),\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    # Candidate label set 6: Combining candidate label set 3 and 5\n",
        "    {\n",
        "        'name': 'Beverage and Separate Receptacle Description',\n",
        "        'labels': [\n",
        "            ('beer', 'beer'),\n",
        "            ('beer bottle', 'beer'),\n",
        "            ('beer glass', 'beer'),\n",
        "            ('wine', 'wine'),\n",
        "            ('wine glass', 'wine'),\n",
        "            ('coffee', 'coffee'),\n",
        "            ('coffee mug', 'coffee'),\n",
        "            ('coffee plunger', 'coffee'),\n",
        "            ('tea', 'tea'),\n",
        "            ('teacup', 'tea'),\n",
        "            ('water', 'water'),\n",
        "            ('water bottle', 'water'),\n",
        "            ('water glass', 'water'),\n",
        "            ('no drinks', 'no drinks'),\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    # Candidate label set 7: Candidate label set 6, with added labels\n",
        "    #                        containing 'person holding' prefixes\n",
        "    {\n",
        "        'name': 'Descriptive Phrases',\n",
        "        'labels': [\n",
        "            ('beer', 'beer'),\n",
        "            ('person holding beer', 'beer'),\n",
        "            ('beer bottle', 'beer'),\n",
        "            ('person holding beer bottle', 'beer'),\n",
        "            ('beer glass', 'beer'),\n",
        "            ('person holding beer glass', 'beer'),\n",
        "            ('wine', 'wine'),\n",
        "            ('person holding wine', 'wine'),\n",
        "            ('wine glass', 'wine'),\n",
        "            ('person holding wine glass', 'wine'),\n",
        "            ('coffee', 'coffee'),\n",
        "            ('person holding coffee', 'coffee'),\n",
        "            ('coffee mug', 'coffee'),\n",
        "            ('person holding coffee mug', 'coffee'),\n",
        "            ('coffee plunger', 'coffee'),\n",
        "            ('person holding coffee plunger', 'coffee'),\n",
        "            ('tea', 'tea'),\n",
        "            ('teacup', 'tea'),\n",
        "            ('person holding teacup', 'tea'),\n",
        "            ('water', 'water'),\n",
        "            ('person holding water', 'water'),\n",
        "            ('water bottle', 'water'),\n",
        "            ('person holding water bottle', 'water'),\n",
        "            ('water glass', 'water'),\n",
        "            ('person holding water glass', 'water'),\n",
        "            ('no drinks', 'no drinks'),\n",
        "        ]\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fPwQW9hM4hL"
      },
      "source": [
        "#### Model Inference\n",
        "\n",
        "The code cell below runs the CLIP model on **ALL** sets of candidate labels and automatically downloads separate CSV files with predictions for each set.\n",
        "\n",
        "Note: This cell may take some time to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SreJF6deM5Q4"
      },
      "outputs": [],
      "source": [
        "# Set the batch size\n",
        "clip_batch_size = 10\n",
        "\n",
        "# Run inference on each label type\n",
        "clip_all_label_results = []\n",
        "for candidate_label_set in clip_label_set:\n",
        "    # Extract information about these candidate labels (including class mapping)\n",
        "    label_set_name = candidate_label_set['name']\n",
        "    candidate_labels, candidate_classes = [list(dat) for dat in zip(*candidate_label_set['labels'])]\n",
        "    unique_candidate_classes = list(dict.fromkeys(candidate_classes))\n",
        "    candidate_label_map = {label: class_name for label, class_name in candidate_label_set['labels']}\n",
        "\n",
        "    # Generate prediction results for all images in the dataset.\n",
        "    current_results = []\n",
        "    progress = tqdm(range(0, len(beverage_dataframe), clip_batch_size))\n",
        "    for index in progress:\n",
        "        progress.set_description(f'Processing for \"{label_set_name}\"... labels')\n",
        "        next_data = beverage_dataframe.iloc[index:index + clip_batch_size]\n",
        "        images = [Image.open('Images/' + sample['image_name'] + '.jpg') for _, sample in next_data.iterrows()]\n",
        "        inputs = clip_processor(text=candidate_labels, images=images, return_tensors='pt', padding=True).to(clip_device)\n",
        "        with torch.no_grad():\n",
        "            outputs = clip_model(**inputs)\n",
        "        image_logits = outputs.logits_per_image.cpu()\n",
        "        image_probs = list(torch.unbind(image_logits.softmax(dim=1), dim=0))\n",
        "        current_results.extend(image_probs)\n",
        "\n",
        "    # Save output to a CSV file.\n",
        "    save_clip_predictions(f'clip_predictions_{label_set_name.lower()}.csv',\n",
        "                          beverage_dataframe, current_results, candidate_labels,\n",
        "                          'beverage_type', label_class_map=candidate_label_map)\n",
        "\n",
        "    # Store a summary of the results\n",
        "    clip_all_label_results.append(current_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dEqeQpq2HdZ"
      },
      "source": [
        "#### Evaluation\n",
        "\n",
        "The code cell below evaluates performance on the validation dataset for each set of candidate labels (similar to Cell 1.4). For every candidate label set, the following will be generated:\n",
        "\n",
        "* A table summarising key per-class metrics, along with metrics aggregated across the entire validation dataset (similar to Cell 1.4a).\n",
        "* A confusion matrix across all beverage classes (similar to Cell 1.4b).\n",
        "\n",
        "Finally, a summary table will be produced showing the overall accuracy and unweighted average recall for each candidate label set, sorted by accuracy. This provides a quick comparison between sets and can help identify which ones may be best suited for application to larger datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwMOmM_xPrnp"
      },
      "outputs": [],
      "source": [
        "# Store for summary statistics (name, accuracy, UAR)\n",
        "clip_summary_statistics = []\n",
        "\n",
        "for candidate_label_set, results in zip(clip_label_set, clip_all_label_results):\n",
        "    label_set_name = candidate_label_set['name']\n",
        "    print(f'Metrics for label set: \"{label_set_name}\"\\n')\n",
        "\n",
        "    # Extract information\n",
        "    candidate_labels, candidate_classes = [list(dat) for dat in zip(*candidate_label_set['labels'])]\n",
        "    unique_candidate_classes = list(dict.fromkeys(candidate_classes))\n",
        "    candidate_label_map = {label: class_name for label, class_name in candidate_label_set['labels']}\n",
        "\n",
        "    # Extract labels\n",
        "    predicted_labels = [candidate_labels[torch.argmax(result)] for result in results]\n",
        "    ground_truth_class = list(beverage_dataframe['beverage_type'])\n",
        "\n",
        "    # Map labels to class\n",
        "    predicted_class = [candidate_label_map[label] for label in predicted_labels]\n",
        "\n",
        "    # Produce the metrics report\n",
        "    generate_metric_report(ground_truth_class, predicted_class, label_names=unique_candidate_classes)\n",
        "\n",
        "    # Display the confusion matrix\n",
        "    ConfusionMatrixDisplay.from_predictions(ground_truth_class, predicted_class, labels=unique_candidate_classes,\n",
        "                                            xticks_rotation='vertical', cmap='Blues', colorbar=False)\n",
        "    plt.show()\n",
        "\n",
        "    # Store information for final summary\n",
        "    clip_summary_statistics.append({\n",
        "        'name': label_set_name,\n",
        "        'accuracy': accuracy_score(ground_truth_class, predicted_class),\n",
        "        'UAR': recall_score(ground_truth_class, predicted_class, labels=unique_candidate_classes,\n",
        "                            average='macro', zero_division=np.nan),\n",
        "    })\n",
        "\n",
        "    # Produce some spacing for the next label set\n",
        "    print('\\n' * 2)\n",
        "    print('-' * 70)\n",
        "\n",
        "\n",
        "# Print final summary ranked by accuracy then UAR\n",
        "clip_summary_statistics.sort(key=lambda x: (x['accuracy'], x['UAR']), reverse=True)\n",
        "len_longest_name = len(max(clip_summary_statistics, key=lambda x: len(x['name']))['name'])\n",
        "print('Final Summary Across Candidate Labels\\n(Sorted by Highest Accuracy)\\n')\n",
        "print(f'{\"Name\":^{len_longest_name}s} | {\"Accuracy\"} | {\"UAR\":^6s}')\n",
        "print('-' * len_longest_name + ' + ' + '-' * 8 + ' + ' + '-' * 6)\n",
        "for summary_stat in clip_summary_statistics:\n",
        "    print(f'{summary_stat[\"name\"]:{len_longest_name}s} | {summary_stat[\"accuracy\"]:^8.2%} | {summary_stat[\"UAR\"]:6.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahzP-2uA02J-"
      },
      "source": [
        "### LLaVA\n",
        "\n",
        "This section evaluates several sets of prompts for the LLaVA model to compare how various sets of prompts impact model performance.\n",
        "\n",
        "Please make sure to run Cell 2.0 and Cell 2.3 before running this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDApQ64_1Ezc"
      },
      "source": [
        "#### Prompts\n",
        "\n",
        "The code cell below defines a list of different prompts that will be passed to LLaVA to make predictions on our validation dataset.\n",
        "\n",
        "Each item in the list is a dictionary with:\n",
        "* `name`: An identifier for the prompt. This name appears when summarising model performance and is used as the filename when storing predictions a CSV.\n",
        "* `prompt`: The prompt given to LLaVA.\n",
        "* `response_map`: A dictionary mapping LLaVA's responses to the corresponding ground truth beverage class.\n",
        "* `default_class`: The class assigned when LLaVA's response is not found in `response_map`. This handles unexpected responses and is typically set to the negative class (e.g. `none`).\n",
        "* `max_new_tokens`: The maximum number of tokens LLaVA is allowed to generate in its response.\n",
        "\n",
        "An example item is shown below:\n",
        "```\n",
        "{\n",
        "    'name': 'Beverage Receptacle',\n",
        "    'prompt': 'What type of beverage does this image contain? Pick the most relevant one from this list: beer bottle, beer glass, wine glass, coffee mug, coffee plunger, teacup, water bottle, water glass, none. Respond with exactly one item from the list.',\n",
        "    'response_map': {\n",
        "        'beer bottle': 'beer',\n",
        "        'beer glass': 'beer',\n",
        "        'wine glass': 'wine',\n",
        "        'coffee mug': 'coffee',\n",
        "        'coffee plunger': 'coffee',\n",
        "        'teacup': 'tea',\n",
        "        'water bottle': 'water',\n",
        "        'water glass': 'water',\n",
        "        'none': 'none',\n",
        "    },\n",
        "    'default_class': 'none',\n",
        "    'max_new_tokens': 10,\n",
        "}\n",
        "```\n",
        "\n",
        "If you would like to explore further prompt engineering on our validation dataset, feel free to add your own prompts below or modify the existing ones as you see fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyRqB8-7vkY4"
      },
      "outputs": [],
      "source": [
        "# Sets of prompts to be evaluated\n",
        "llava_prompt_set = [\n",
        "    # Prompt 1: Asking for a beverage name in a list\n",
        "    {\n",
        "        'name': 'Beverage name',\n",
        "        'prompt': 'What type of beverage does this image contain? Pick the most relevant one from this list: beer, wine, coffee, tea, water, none. Answer in one word.',\n",
        "        'response_map': {\n",
        "            'beer': 'beer',\n",
        "            'wine': 'wine',\n",
        "            'coffee': 'coffee',\n",
        "            'tea': 'tea',\n",
        "            'water': 'water',\n",
        "            'none': 'none',\n",
        "        },\n",
        "        'default_class': 'none',\n",
        "        'max_new_tokens': 10,\n",
        "    },\n",
        "\n",
        "    # Prompt 2: Asking for conservative predictions on the alcohol classes\n",
        "    {\n",
        "        'name': 'Beverage name Alcohol Conservative',\n",
        "        'prompt': 'What type of beverage does this image contain? Pick the most relevant one from this list: beer, wine, coffee, tea, water, none. Only predict beer or wine if you are completely confident. Answer in one word.',\n",
        "        'response_map': {\n",
        "            'beer': 'beer',\n",
        "            'wine': 'wine',\n",
        "            'coffee': 'coffee',\n",
        "            'tea': 'tea',\n",
        "            'water': 'water',\n",
        "            'none': 'none',\n",
        "        },\n",
        "        'default_class': 'none',\n",
        "        'max_new_tokens': 10,\n",
        "    },\n",
        "\n",
        "    # Prompt 3: Adding separate receptacle descriptions in the prompt\n",
        "    {\n",
        "        'name': 'Beverage Receptacle',\n",
        "        'prompt': 'What type of beverage does this image contain? Pick the most relevant one from this list: beer bottle, beer glass, wine glass, coffee mug, coffee plunger, teacup, water bottle, water glass, none. Respond with exactly one item from the list.',\n",
        "        'response_map': {\n",
        "            'beer bottle': 'beer',\n",
        "            'beer glass': 'beer',\n",
        "            'wine glass': 'wine',\n",
        "            'coffee mug': 'coffee',\n",
        "            'coffee plunger': 'coffee',\n",
        "            'teacup': 'tea',\n",
        "            'water bottle': 'water',\n",
        "            'water glass': 'water',\n",
        "            'none': 'none',\n",
        "        },\n",
        "        'default_class': 'none',\n",
        "        'max_new_tokens': 10,\n",
        "    },\n",
        "\n",
        "    # Prompt 4: Same as prompt 3, but also adding the beverage name\n",
        "    {\n",
        "        'name': 'Beverage Type and Receptacle',\n",
        "        'prompt': 'What type of beverage does this image contain? Pick the most relevant one from this list: beer, beer bottle, beer glass, wine, red wine, white wine, wine glass, coffee, coffee mug, coffee plunger, tea, teacup, water, water bottle, water glass, none. Respond with exactly one item from the list.',\n",
        "        'response_map': {\n",
        "            'beer': 'beer',\n",
        "            'beer bottle': 'beer',\n",
        "            'beer glass': 'beer',\n",
        "            'wine': 'wine',\n",
        "            'red wine': 'wine',\n",
        "            'white wine': 'wine',\n",
        "            'wine glass': 'wine',\n",
        "            'coffee': 'coffee',\n",
        "            'coffee mug': 'coffee',\n",
        "            'coffee plunger': 'coffee',\n",
        "            'tea': 'tea',\n",
        "            'teacup': 'tea',\n",
        "            'water': 'water',\n",
        "            'water bottle': 'water',\n",
        "            'water glass': 'water',\n",
        "            'none': 'none',\n",
        "        },\n",
        "        'default_class': 'none',\n",
        "        'max_new_tokens': 10,\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7VupT41-LzU"
      },
      "source": [
        "#### Model Inference\n",
        "\n",
        "The code cell below runs the LLaVA model on **ALL** prompts and automatically downloads separate CSV files with predictions for each prompt.\n",
        "\n",
        "Note: This cell may take some time to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuA16JDO-Mqh"
      },
      "outputs": [],
      "source": [
        "# Set the batch size\n",
        "llava_batch_size = 10\n",
        "\n",
        "# Run inference on each prompt set\n",
        "llava_all_prompt_results = []\n",
        "for prompt_set in llava_prompt_set:\n",
        "    # Extract information about these prompts (including response mapping)\n",
        "    prompt_set_name = prompt_set['name']\n",
        "    current_llava_prompt = prompt_set['prompt']\n",
        "    current_llava_max_tokens = prompt_set['max_new_tokens']\n",
        "    response_map = {k.lower(): v.lower() for k, v in prompt_set['response_map'].items()}\n",
        "    candidate_classes = list(prompt_set['response_map'].values()) + [prompt_set['default_class']]\n",
        "    unique_candidate_classes = list(dict.fromkeys(candidate_classes))\n",
        "\n",
        "    # Generate prediction results for all images in the dataset.\n",
        "    current_results = []\n",
        "    progress = tqdm(range(0, len(beverage_dataframe), llava_batch_size))\n",
        "    for index in progress:\n",
        "        progress.set_description(f'Processing for \"{prompt_set_name}\"... prompt')\n",
        "        next_data = beverage_dataframe.iloc[index:index + llava_batch_size]\n",
        "        images = [Image.open('Images/' + sample['image_name'] + '.jpg') for _, sample in next_data.iterrows()]\n",
        "        llava_prompt = f'[INST] <image>\\n{current_llava_prompt} [/INST]'\n",
        "        inputs = llava_processor(text=[llava_prompt] * len(images), images=images, return_tensors='pt').to(llava_device)\n",
        "        with torch.no_grad():\n",
        "            outputs = llava_model.generate(**inputs, max_new_tokens=current_llava_max_tokens)\n",
        "        for output in outputs:\n",
        "            generated_text = llava_processor.decode(output.cpu(), skip_special_tokens=True)\n",
        "            response = generated_text.split(f'{current_llava_prompt} [/INST]')[1].strip().lower()\n",
        "            current_results.append(response)\n",
        "\n",
        "    # Save output to a CSV file.\n",
        "    save_llava_predictions(f'llava_predictions_{prompt_set_name.lower()}.csv',\n",
        "                           beverage_dataframe, current_results, 'beverage_type',\n",
        "                           response_class_map=response_map,\n",
        "                           default_class=prompt_set['default_class'])\n",
        "\n",
        "    # Store a summary of the results\n",
        "    llava_all_prompt_results.append(current_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqb-brCNCFsy"
      },
      "source": [
        "#### Evaluation\n",
        "\n",
        "The code cell below evaluates performance on the validation dataset for each prompt (similar to Cell 2.4). For every prompt, the following will be generated:\n",
        "\n",
        "* A table summarising key per-class metrics, along with metrics aggregated across the entire validation dataset (similar to Cell 2.4a).\n",
        "* A confusion matrix across all beverage classes (similar to Cell 2.4b).\n",
        "\n",
        "Finally, a summary table will be produced showing the overall accuracy and unweighted average recall for each prompt, sorted by accuracy. This provides a quick comparison between prompts and can help identify which ones may be best suited for application to larger datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbssn15G-_7_"
      },
      "outputs": [],
      "source": [
        "# Store for summary statistics (name, accuracy, UAR)\n",
        "llava_summary_statistics = []\n",
        "\n",
        "for prompt_set, results in zip(llava_prompt_set, llava_all_prompt_results):\n",
        "    prompt_set_name = prompt_set['name']\n",
        "    print(f'Metrics for prompt: \"{prompt_set_name}\"\\n')\n",
        "\n",
        "    # Extract information\n",
        "    response_map = {k.lower(): v.lower() for k, v in prompt_set['response_map'].items()}\n",
        "    candidate_classes = list(prompt_set['response_map'].values()) + [prompt_set['default_class']]\n",
        "    unique_candidate_classes = list(dict.fromkeys(candidate_classes))\n",
        "\n",
        "    # Extract labels\n",
        "    predicted_labels = results\n",
        "    ground_truth_class = list(beverage_dataframe['beverage_type'])\n",
        "\n",
        "    # Map labels to class\n",
        "    predicted_class = [response_map.get(label, prompt_set['default_class']) for label in predicted_labels]\n",
        "\n",
        "    # Produce the metrics report\n",
        "    generate_metric_report(ground_truth_class, predicted_class, label_names=unique_candidate_classes)\n",
        "\n",
        "    # Display the confusion matrix\n",
        "    ConfusionMatrixDisplay.from_predictions(ground_truth_class, predicted_class, labels=unique_candidate_classes,\n",
        "                                            xticks_rotation='vertical', cmap='Blues', colorbar=False)\n",
        "    plt.show()\n",
        "\n",
        "    # Store information for final summary\n",
        "    llava_summary_statistics.append({\n",
        "        'name': prompt_set_name,\n",
        "        'accuracy': accuracy_score(ground_truth_class, predicted_class),\n",
        "        'UAR': recall_score(ground_truth_class, predicted_class, labels=unique_candidate_classes,\n",
        "                            average='macro', zero_division=np.nan),\n",
        "    })\n",
        "\n",
        "    # Produce some spacing for the next label set\n",
        "    print('\\n' * 2)\n",
        "    print('-' * 70)\n",
        "\n",
        "\n",
        "# Print final summary ranked by accuracy then UAR\n",
        "llava_summary_statistics.sort(key=lambda x: (x['accuracy'], x['UAR']), reverse=True)\n",
        "len_longest_name = len(max(llava_summary_statistics, key=lambda x: len(x['name']))['name'])\n",
        "print('Final Summary Across Prompts\\n(Sorted by Highest Accuracy)\\n')\n",
        "print(f'{\"Name\":^{len_longest_name}s} | {\"Accuracy\"} | {\"UAR\":^6s}')\n",
        "print('-' * len_longest_name + ' + ' + '-' * 8 + ' + ' + '-' * 6)\n",
        "for summary_stat in llava_summary_statistics:\n",
        "    print(f'{summary_stat[\"name\"]:{len_longest_name}s} | {summary_stat[\"accuracy\"]:^8.2%} | {summary_stat[\"UAR\"]:6.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGWQUTLw0-Ri"
      },
      "source": [
        "## Try Your Own Analysis\n",
        "\n",
        "When running the following cell, you will be asked to select/input your data file using widgets that appear directly below the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1LmMqZjp1QY3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # If we are on Google Colab, show an upload widget.\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        file_locations = list(uploaded.keys())\n",
        "    else:\n",
        "        file_locations = ''\n",
        "except ModuleNotFoundError:\n",
        "    # If we are not on Google Colab, ask for the names of all files.\n",
        "    file_locations = []\n",
        "    while True:\n",
        "        next_location = input('Please enter the path to the image(s) you would like to classify. '\n",
        "                              'Type \\q to finish entering filepaths.')\n",
        "        if next_location == '\\q':\n",
        "            break\n",
        "\n",
        "for file_path in file_locations:\n",
        "    if not os.path.isfile(file_path):\n",
        "        print(f'File not found: {file_path}.')\n",
        "        print('Please run this cell again.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nOHTP7m1gBh"
      },
      "source": [
        "### CLIP\n",
        "\n",
        "*Ensure that you have run through all previous code cells in the \"Example 1\" section first, as this code makes use of the classifier we initialised in that part of the tutorial.*\n",
        "\n",
        "For simplicity, the code below does *not* use batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNeFQcqW2cKR"
      },
      "outputs": [],
      "source": [
        "# Here we specify the label options that the model will choose from.\n",
        "# Make sure that you update these options to suit your data and experiment\n",
        "# with different wordings.\n",
        "candidate_labels = [\n",
        "    'A photo containing beer',\n",
        "    'A photo containing wine',\n",
        "    'A photo containing water',\n",
        "    'A photo containing coffee',\n",
        "    'A photo containing tea',\n",
        "    'A photo containing no drinks',\n",
        "]\n",
        "\n",
        "# Put into a dataframe for compatibility with the previous code.\n",
        "custom_clip_dataframe = pd.DataFrame(file_locations, columns=['image_name'])\n",
        "\n",
        "# Generate prediction results.\n",
        "all_results = []\n",
        "for index, sample in tqdm(custom_clip_dataframe.iterrows(), total=len(custom_clip_dataframe)):\n",
        "    image = Image.open(sample['image_name'])\n",
        "    inputs = clip_processor(text=candidate_labels, images=image, return_tensors='pt', padding=True).to(clip_device)\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "    image_logits = outputs.logits_per_image[0].cpu()\n",
        "    image_probs = image_logits.softmax(dim=0)\n",
        "    all_results.append(image_probs)\n",
        "\n",
        "# Save output to a CSV file.\n",
        "save_clip_predictions('clip_predictions.csv', custom_clip_dataframe, all_results, candidate_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-TnDnEo1iWd"
      },
      "source": [
        "### LLaVA\n",
        "\n",
        "*Ensure that you have run through all previous code cells in the \"Example 2\" section first, as this code makes use of the classifier we initialised in that part of the tutorial.*\n",
        "\n",
        "For simplicity, the code below does *not* use batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0WdyLMs2tML"
      },
      "outputs": [],
      "source": [
        "# Here we specify the prompt for the model.\n",
        "# Make sure that you update the prompt to suit your data and experiment with\n",
        "# different wordings.\n",
        "prompt = 'What type of beverage does this image contain? Pick the most relevant one from this list: beer, wine, coffee, tea, water, none. Answer in one word.'\n",
        "\n",
        "# Here we specify how many tokens (words or word parts) can be returned by\n",
        "# LLaVA. Increase this limit to enable the model to give a more detailed response,\n",
        "# depending on the prompt.\n",
        "max_new_tokens = 10\n",
        "\n",
        "# Put into a dataframe for compatibility with the previous code.\n",
        "custom_llava_dataframe = pd.DataFrame(file_locations, columns=['image_name'])\n",
        "\n",
        "# Generate prediction results.\n",
        "all_results = []\n",
        "for index, sample in tqdm(custom_llava_dataframe.iterrows(), total=len(custom_llava_dataframe)):\n",
        "    image = Image.open(sample['image_name'])\n",
        "    llava_prompt = f'[INST] <image>\\n{prompt} [/INST]'\n",
        "    inputs = llava_processor(text=llava_prompt, images=image, return_tensors='pt').to(llava_device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llava_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    generated_text = llava_processor.decode(outputs[0].cpu(), skip_special_tokens=True)\n",
        "    response = generated_text.split(f'{prompt} [/INST]')[1].strip().lower()\n",
        "    all_results.append(response)\n",
        "\n",
        "# Save output to a CSV file.\n",
        "save_llava_predictions('llava_predictions.csv', custom_llava_dataframe, all_results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}